# Term-project
# 수어 인식 기반 실시간 해석 프로그램

## 📌 프로젝트 개요

이 프로젝트는 **한 손 수어 동작을 실시간으로 인식하고 해당 의미를 화면에 말풍선 형태로 출력**하는 수어 인식 인터페이스입니다. 사용자는 직접 수어 데이터를 수집하고 모델을 재학습시킨 후, 실시간으로 카메라를 통해 수어를 인식하고 자막처럼 출력되는 결과를 확인할 수 있습니다.

---

## ✅ 핵심 기능

| 기능           | 설명                                                  |
| ------------ | --------------------------------------------------- |
| 📸 수어 수집     | GUI를 통해 원하는 수어 단어를 입력하고, 10개의 시퀀스를 자동 수집 (30프레임 기준) |
| 🧠 모델 재학습    | 수집된 `.npy` 파일들을 기반으로 LSTM 모델 자동 재학습                 |
| 🟢 실시간 수어 해석 | 학습된 모델을 기반으로 수어 동작을 인식하고 의미를 화면에 출력 (말풍선 형태)        |
| 🖥 GUI 인터페이스 | `.bat` 없이도 직관적인 수어 추가 + 실시간 인식 기능 제공                |

---

## 💻 실행 방법

```bash
# 1. 가상환경 실행
./venv/Scripts/activate

# 2. GUI 실행
python gui_launcher.py
```

---

## 🗂 프로젝트 파일 구조 및 역할

```
game_project/
├── gui_launcher.py          # GUI 인터페이스: 수어 수집 및 실시간 인식 실행 버튼 포함
├── main.py                  # 실시간 수어 인식: 카메라로 손동작 인식, 의미 예측 후 말풍선 출력
├── batch_collector.py       # 단일 수어 시퀀스 수집기 (30프레임 x 10세트)
├── auto_retrain.py          # .npy 데이터를 기반으로 LSTM 재학습 및 모델 저장
├── model/
│   ├── model.pth            # 학습된 PyTorch LSTM 모델 저장
│   ├── trainer.py           # 학습 로직 및 LSTM 모델 클래스 정의
│   └── predictor.py         # 실시간 예측용 로직 (필요 시 확장 가능)
├── utils/
│   └── landmark_utils.py    # 손 랜드마크 좌표 정규화 및 전처리 함수
├── hand_tracker.py          # Mediapipe 기반 손 랜드마크 추적 클래스 정의
├── fonts/
│   └── NanumGothicBold.ttf  # PIL 텍스트 출력을 위한 한글 폰트
├── data/                    # 수집된 수어 시퀀스(npy) 저장 폴더
└── requirements.txt         # 의존 패키지 목록
```

---

## 🔍 주요 코드 파일별 주요 함수 및 설명

### `gui_launcher.py`

* `collect_and_train()` : 입력된 수어 이름으로 `batch_collector.py` 실행 → 이후 `auto_retrain.py` 실행
* `run_main()` : 실시간 수어 해석 실행 (main.py)
* GUI 구성: Tkinter로 구성된 수어 입력창, 수집 버튼, 실시간 실행 버튼 포함 (스타일 포함)

### `main.py`

* `draw_korean_text()` : PIL을 활용하여 말풍선 내부에 한글 텍스트 출력
* `draw_speech_bubble()` : 말풍선을 적절한 위치에 곡선 배경과 함께 출력하며 tail도 추가됨
* 실시간 카메라 입력 → 랜드마크 추출 → LSTM 예측 → 화면에 출력 (텍스트가 일정 신뢰도 이상일 경우에만 출력)

### `batch_collector.py`

* `collect_sequence(label)` : 카메라 프레임에서 손 랜드마크를 추출하고 30프레임 시퀀스를 저장
* GUI 환경에서 자동 실행되며, 이미 저장된 데이터는 덮어쓰지 않음 (데이터 보존)

### `auto_retrain.py`

* `SignLanguageDataset` : `.npy` 데이터셋을 PyTorch용으로 불러오는 Dataset 클래스
* `LSTMClassifier` : LSTM 모델 정의 (input\_size=63, hidden\_size=128, output\_size는 수어 개수에 따라 자동 설정)
* `train_model()` : 전체 `.npy` 데이터를 기반으로 모델 학습 → `model.pth` 저장

### `trainer.py`

* `LSTMClassifier` : LSTM 아키텍처 정의 (다층 RNN 구성 가능)
* `train_model()` : 전체 데이터를 불러와 Epoch 단위로 학습 및 평가

### `predictor.py`

* `predict_sequence()` : 입력된 시퀀스 데이터로부터 수어 예측 결과 반환
* softmax confidence 기반 필터링 가능

### `hand_tracker.py`

* `HandTracker.get_hand_landmarks()` : Mediapipe로 손 인식
* `get_landmark_array()` : 21개 손 관절 좌표를 63차원 벡터로 변환
* `draw_landmarks()` : 인식된 손에 랜드마크 시각화

### `landmark_utils.py`

* `normalize_landmarks()` : 랜드마크를 상대 위치로 정규화하여 일반화된 입력 형식 생성

---

## 📚 사용된 주요 기술

* Python 3.x
* OpenCV
* MediaPipe
* PyTorch (LSTM)
* Tkinter (GUI)
* PIL (한글 텍스트 시각화)

---

## ✋ 현재 포함된 수어 리스트 (2025.05 기준)

* 네 (응, 맞아, 정말)
* 아니
* 감사합니다
* 즐겁다
* 좋다
* 나쁘다
* 안녕하세요
* 사랑해요
* 만나다 ← 예시로 GUI로 새로 추가

> 모든 수어는 30프레임 x 10세트로 구성됩니다.

---

## ⚠️ 한계점 및 힘들었던 점

| 항목          | 설명                                                          |
| ----------- | ----------------------------------------------------------- |
| 인식 정확도      | 데이터 수가 적으면 오인식 발생 가능                                        |
| 손 외 환경 변수   | 배경, 조명, 거리의 영향을 받음                                          |
| 단일 손 기반     | 양손 수어는 미지원 (오른손만 추적)                                        |
| 모델 저장 오류    | `label_encoder` 포함 시 `torch.load()` 오류 발생 (UnpicklingError) |
| GUI 환경 오류   | 가상환경 경로 설정 안 되어 있을 경우 `torch` 인식 실패                         |
| 불확실한 인식 필터링 | confidence가 낮은 경우 말풍선 미출력 기능 구현 (유저 요청 반영)                  |

---

## 🔮 향후 발전 방향

| 항목               | 설명                                     |
| ---------------- | -------------------------------------- |
| ✅ 양손 인식 확장       | Mediapipe에서 왼손/오른손 모두 추적 처리            |
| ✅ 수어 문장 인식       | 단어 조합을 통한 간단한 문장 해석 (예: "안녕하세요 감사합니다") |
| ✅ confidence 시각화 | 예측 확신도가 낮을 경우 흐리게 출력하거나 생략             |
| ✅ 웹 기반 앱화        | Streamlit 또는 Flask 기반 웹 수어 인식 시스템      |
| ✅ 데이터 증강         | 수어마다 다양한 손모양/속도로 수집 후 증강 기법 적용         |

🔧 향후 발전 방향 (Future Improvements)
본 프로젝트는 현재 개인의 실시간 캠 기반 수어 인식 시스템으로 구축되었으며, 향후 다음과 같은 확장이 가능합니다:

1. 영상통화 기반 실시간 수어 통역 시스템
WebRTC 또는 Zoom SDK 등을 활용하여 영상 통화 중 수어를 인식하고 자동 통역하는 기능.

영상통화 참여자 간에 음성/수어의 양방향 변환 가능성 제시.

2. 멀티유저 수어 인식
여러 사용자의 손 제스처를 동시에 인식하는 기능 (멀티 핸드 트래킹).

다양한 카메라 각도 및 배경에서도 견고하게 동작하도록 일반화된 인식 모델 도입.

3. 텍스트 음성 합성 (TTS) 연동
인식된 수어를 단순 텍스트 출력에 그치지 않고, 음성으로 변환하여 청각장애인 외 일반 사용자와의 원활한 의사소통 가능.

4. 모바일 또는 웹 기반 시스템으로 확장
React Native, Flutter, 또는 WebAssembly 기반으로 시스템 포팅.

브라우저 기반 수어 인식 서비스로의 확장 고려.

5. 문장 수준의 연속 수어 인식
현재 단일 수어 단어 인식에서 나아가, 시간적 흐름을 고려한 문장 단위 수어 인식 기능 구현.

LSTM, Transformer 기반 시계열 문장 해석 고도화.
---

## 🙋‍♀️ 어려웠던 점 및 해결 사례

* PyTorch `torch.save()` → `torch.load()` 간 label\_encoder 포함 시 Unpickling 오류 발생 → 해결: `weights_only=False` 옵션 또는 encoder 따로 저장
* GUI에서 `python` 실행 시 venv 경로가 무시됨 → 해결: `sys.executable` 또는 `venv/Scripts/python.exe` 경로를 명시
* 손의 작은 흔들림도 다른 수어로 인식됨 → 해결: 최근 예측 값을 5초간 유지하도록 보완
* confidence 낮은 입력에 대해서도 텍스트가 출력됨 → 해결: confidence < 0.8일 경우 말풍선 출력 생략

---

## 🧾 LICENSE / 활용 목적

본 프로젝트는 교육 목적 및 개인 실습용으로 작성되었습니다.
추후 논문, 졸업작품, 시연 영상, 캡스톤디자인 등에 자유롭게 활용 가능하며 상업적 이용은 별도 문의 바랍니다.

---

## ✨ 마무리

이 프로젝트는 수어를 손쉽게 인식하고 누구나 수어 데이터를 직접 추가할 수 있는 구조를 갖춘 실시간 인식 도구입니다.
GUI 기반으로 사용자 접근성을 높였으며, 향후 다양한 기능 확장도 염두에 두고 설계되었습니다.

🤝 **"수어는 언어입니다"** — 모두가 더 나은 소통을 위한 기술에 접근할 수 있기를 바랍니다.
